{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c001847a-ce0d-455a-8265-2e2bd1d27cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cb0ffd-1967-4070-a03d-ab826d31b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train.training_utils import load_datasets, evaluate\n",
    "from ap.averaged_perceptron import AveragedPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "258fc2c2-44f1-4630-919e-c46f0a020e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(features, prev_label, prev_label2):\n",
    "\n",
    "    prepared_features = set()\n",
    "    for key, value in features.items():\n",
    "        if isinstance(value, bool) and value:\n",
    "            prepared_features.add(key)\n",
    "        elif isinstance(value, str):\n",
    "            prepared_features.add(key + \"=\" + value)\n",
    "\n",
    "    #prepared_features.add(\"prev_label+stem=\" + prev_label + \"+\" + features[\"stem\"])\n",
    "    prepared_features.add(\"prev_label=\" + prev_label)\n",
    "    prepared_features.add(\"prev_label2=\" + prev_label2)\n",
    "    #prepared_features.add(\"prev_label+prev_label2=\" + prev_label + \"+\" + prev_label2)\n",
    "    return prepared_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b6eb5b-9dba-49f3-939d-1fb4badc6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(features):\n",
    "\n",
    "    labels = []\n",
    "    for i, feat in enumerate(features):\n",
    "        if i == 0:\n",
    "            prev_label, prev_label2 = \"-START-\", \"-START2-\"\n",
    "\n",
    "        feats = prepare_features(feat, prev_label, prev_label2)\n",
    "        labels.append(model.predict(feats))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d553ed-3873-473b-bdc1-394fd77e8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading and transforming training data.\n",
      "[INFO] 59,934 usable vectors.\n",
      "[INFO] 66 discarded due to OTHER labels.\n"
     ]
    }
   ],
   "source": [
    "vectors = load_datasets(\"../train/data/training.sqlite3\", \"en\", [\"bbc\", \"cookstr\", \"nyt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a05ac70-34c0-40e2-b6d7-d723a394e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sentences_train,\n",
    "    sentences_test,\n",
    "    features_train,\n",
    "    features_test,\n",
    "    truth_train,\n",
    "    truth_test,\n",
    "    source_train,\n",
    "    source_test,\n",
    ") = train_test_split(\n",
    "    vectors.sentences,\n",
    "    vectors.features,\n",
    "    vectors.labels,\n",
    "    vectors.source,\n",
    "    test_size=0.2,\n",
    "    stratify=vectors.source,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7adbc5da-6c2f-4a58-a2c5-fc174ae5517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: 327523/342342=95.7%\n",
      "Iter 1: 331246/342342=96.8%\n",
      "Iter 2: 332515/342342=97.1%\n",
      "Iter 3: 333382/342342=97.4%\n",
      "Iter 4: 333686/342342=97.5%\n",
      "CPU times: user 27 s, sys: 7.95 ms, total: 27 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AveragedPerceptron()\n",
    "model.labels = {\"QTY\", \"UNIT\", \"NAME\", \"PREP\", \"COMMENT\", \"PURPOSE\", \"PUNC\", \"SIZE\"}\n",
    "\n",
    "training = list(zip(features_train, truth_train))\n",
    "\n",
    "for iter_ in range(5):\n",
    "    c = 0  # number of correctly labelled tokens this iteration\n",
    "    n = 0  # numer of total tokens this iteration\n",
    "    for sentence_features, sentence_labels in training:\n",
    "        for i, (features, label) in enumerate(zip(sentence_features, sentence_labels)):\n",
    "            if i == 0:\n",
    "                prev_label, prev_label2 = \"-START-\", \"-START2-\"\n",
    "                \n",
    "            feats = prepare_features(features, prev_label, prev_label2)\n",
    "            guess = model.predict(feats)\n",
    "            model.update(label, guess, feats)\n",
    "\n",
    "            prev_label2 = prev_label\n",
    "            # Use the guess here to avoid to model becoming over-reliant on the historical labels\n",
    "            # being correct\n",
    "            prev_label = guess\n",
    "\n",
    "            c += guess == label\n",
    "            n += 1\n",
    "\n",
    "    print(f\"Iter {iter_}: {c}/{n}={100*c/n:.1f}%\")\n",
    "\n",
    "    random.shuffle(training)\n",
    "model.average_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35253eab-422e-46eb-9d17-58aa10efbf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-level results:\n",
      "\tAccuracy: 83.51%\n",
      "\n",
      "Word-level results:\n",
      "\tAccuracy 95.10%\n",
      "\tPrecision (micro) 95.46%\n",
      "\tRecall (micro) 95.10%\n",
      "\tF1 score (micro) 95.21%\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for sentence_features, sentence_labels in zip(features_test, truth_test):\n",
    "    true_labels.append(sentence_labels)\n",
    "    predicted_labels.append(tag(sentence_features))\n",
    "\n",
    "stats = evaluate(true_labels, predicted_labels)\n",
    "print(\"Sentence-level results:\")\n",
    "print(f\"\\tAccuracy: {100*stats.sentence.accuracy:.2f}%\")\n",
    "\n",
    "print()\n",
    "print(\"Word-level results:\")\n",
    "print(f\"\\tAccuracy {100*stats.token.accuracy:.2f}%\")\n",
    "print(f\"\\tPrecision (micro) {100*stats.token.weighted_avg.precision:.2f}%\")\n",
    "print(f\"\\tRecall (micro) {100*stats.token.weighted_avg.recall:.2f}%\")\n",
    "print(f\"\\tF1 score (micro) {100*stats.token.weighted_avg.f1_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d494322d-3b3c-4308-8c17-3c55e7be1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ap.pickle\", \"wb\") as f:\n",
    "    pickle.dump((model.weights, model.labels), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd5013d-d46d-4bf7-98e2-3d04a0ba5d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NAME': 2.52, 'UNIT': 0.849, 'SIZE': -1.0, 'PREP': 1.201, 'COMMENT': -3.569}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights[\"stem=clove\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f598783-09c3-4898-a7bb-c1cb0fb6509f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
