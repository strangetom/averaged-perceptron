{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c001847a-ce0d-455a-8265-2e2bd1d27cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cb0ffd-1967-4070-a03d-ab826d31b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "from ingredient_parser.en import PreProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from train.training_utils import load_datasets, evaluate\n",
    "from ap.averaged_perceptron import AveragedPerceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258fc2c2-44f1-4630-919e-c46f0a020e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(\n",
    "    features: dict[str, str | bool], prev_label: str, prev_label2: str\n",
    ") -> set[str]:\n",
    "    \"\"\"Convert features dict to set of strings.\n",
    "\n",
    "    The model weight use the features as keys, so they need to be a string rather\n",
    "    than a key: value pair.\n",
    "    For string features, the string is prepared by joining the key and value by \"=\".\n",
    "    For boolean features, the string is prepared just using the key.\n",
    "\n",
    "    Additional features are added based on the labels of the previous two tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : dict[str, str | bool]\n",
    "        Dictionary of features for token, obtained from PreProcessor.sentence_features()\n",
    "    prev_label : str\n",
    "        Label of previous token\n",
    "    prev_label2 : str\n",
    "        Label of token before previous token\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set\n",
    "        Set of features as strings\n",
    "    \"\"\"\n",
    "    prepared_features = set()\n",
    "    for key, value in features.items():\n",
    "        if isinstance(value, bool) and value:\n",
    "            prepared_features.add(key)\n",
    "        elif isinstance(value, str):\n",
    "            prepared_features.add(key + \"=\" + value)\n",
    "\n",
    "    # Add extra features based on labels of previous tokens.\n",
    "    prepared_features.add(\"prev_label=\" + prev_label)\n",
    "    prepared_features.add(\"prev_label2=\" + prev_label2)\n",
    "    prepared_features.add(\"prev_label+prev_label2=\" + prev_label + \"+\" + prev_label2)\n",
    "    prepared_features.add(\"prev_label+stem=\" + prev_label + \"+\" + features[\"stem\"])\n",
    "\n",
    "    return prepared_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1b6eb5b-9dba-49f3-939d-1fb4badc6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(model: AveragedPerceptron, features: list[dict[str, str | bool]]) -> list[str]:\n",
    "    \"\"\"Tag a list of features dicts with labels using Averaged Perceptron model.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : AveragedPerceptron\n",
    "        Model to use to tag the list of features.\n",
    "    features : list[dict[str, str | bool]]\n",
    "        List of dicts of features for all tokens in sentence,\n",
    "        obtained from PreProcessor.sentence_features()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        List of labels for input tokens.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for i, feat in enumerate(features):\n",
    "        if i == 0:\n",
    "            prev_label, prev_label2 = \"-START-\", \"-START2-\"\n",
    "\n",
    "        feats = prepare_features(feat, prev_label, prev_label2)\n",
    "        label = model.predict(feats)\n",
    "        labels.append(label)\n",
    "\n",
    "        prev_label2 = prev_label\n",
    "        prev_label = label\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ed5cd-80b5-4aef-a619-5dbfec1360a4",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Load training data from database and split into between train and test sets.\n",
    "80% of the data is used for training.\n",
    "20% of the data is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a05ac70-34c0-40e2-b6d7-d723a394e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading and transforming training data.\n",
      "[INFO] 59,933 usable vectors.\n",
      "[INFO] 67 discarded due to OTHER labels.\n"
     ]
    }
   ],
   "source": [
    "vectors = load_datasets(\n",
    "    \"../../ingredient-parser/train/data/training.sqlite3\",\n",
    "    \"en\",\n",
    "    [\"bbc\", \"cookstr\", \"nyt\"],\n",
    ")\n",
    "(\n",
    "    sentences_train,\n",
    "    sentences_test,\n",
    "    features_train,\n",
    "    features_test,\n",
    "    truth_train,\n",
    "    truth_test,\n",
    "    source_train,\n",
    "    source_test,\n",
    ") = train_test_split(\n",
    "    vectors.sentences,\n",
    "    vectors.features,\n",
    "    vectors.labels,\n",
    "    vectors.source,\n",
    "    test_size=0.2,\n",
    "    stratify=vectors.source,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bb6a7-7d0a-4452-96ac-0efacf953192",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7adbc5da-6c2f-4a58-a2c5-fc174ae5517e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: 327796/341927=95.9%\n",
      "Iter 1: 331643/341927=97.0%\n",
      "Iter 2: 332912/341927=97.4%\n",
      "Iter 3: 333370/341927=97.5%\n",
      "Iter 4: 334084/341927=97.7%\n",
      "Iter 5: 334513/341927=97.8%\n",
      "Iter 6: 334821/341927=97.9%\n",
      "Iter 7: 335257/341927=98.0%\n",
      "Iter 8: 335424/341927=98.1%\n",
      "Iter 9: 335680/341927=98.2%\n",
      "CPU times: user 1min 1s, sys: 44.5 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AveragedPerceptron()\n",
    "model.labels = {\"QTY\", \"UNIT\", \"NAME\", \"PREP\", \"COMMENT\", \"PURPOSE\", \"PUNC\", \"SIZE\"}\n",
    "\n",
    "training = list(zip(features_train, truth_train))\n",
    "\n",
    "for iter_ in range(10):\n",
    "    c = 0  # number of correctly labelled tokens this iteration\n",
    "    n = 0  # numer of total tokens this iteration\n",
    "    for sentence_features, sentence_labels in training:\n",
    "        for i, (features, label) in enumerate(zip(sentence_features, sentence_labels)):\n",
    "            if i == 0:\n",
    "                prev_label, prev_label2 = \"-START-\", \"-START2-\"\n",
    "\n",
    "            feats = prepare_features(features, prev_label, prev_label2)\n",
    "            guess = model.predict(feats)\n",
    "            model.update(label, guess, feats)\n",
    "\n",
    "            prev_label2 = prev_label\n",
    "            # Use the guess here to avoid to model becoming over-reliant on the historical labels\n",
    "            # being correct\n",
    "            prev_label = guess\n",
    "\n",
    "            c += guess == label\n",
    "            n += 1\n",
    "\n",
    "    print(f\"Iter {iter_}: {c}/{n}={100*c/n:.1f}%\")\n",
    "\n",
    "    random.shuffle(training)\n",
    "model.average_weights()\n",
    "\n",
    "model.save(\"ap.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af0ab6-18fe-4a05-94c2-4f25baac1dca",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "Evaluate model performance using test data. This is data the model was not trained on, so is representative of how the model will perform when used in the wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35253eab-422e-46eb-9d17-58aa10efbf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-level results:\n",
      "\tAccuracy: 93.55%\n",
      "\n",
      "Word-level results:\n",
      "\tAccuracy 97.72%\n",
      "\tPrecision (micro) 97.76%\n",
      "\tRecall (micro) 97.72%\n",
      "\tF1 score (micro) 97.73%\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for sentence_features, sentence_labels in zip(features_test, truth_test):\n",
    "    true_labels.append(sentence_labels)\n",
    "    predicted_labels.append(tag(model, sentence_features))\n",
    "\n",
    "stats = evaluate(true_labels, predicted_labels)\n",
    "print(\"Sentence-level results:\")\n",
    "print(f\"\\tAccuracy: {100*stats.sentence.accuracy:.2f}%\")\n",
    "\n",
    "print()\n",
    "print(\"Word-level results:\")\n",
    "print(f\"\\tAccuracy {100*stats.token.accuracy:.2f}%\")\n",
    "print(f\"\\tPrecision (micro) {100*stats.token.weighted_avg.precision:.2f}%\")\n",
    "print(f\"\\tRecall (micro) {100*stats.token.weighted_avg.recall:.2f}%\")\n",
    "print(f\"\\tF1 score (micro) {100*stats.token.weighted_avg.f1_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a8f5adc-59bb-48fa-b120-5c37c6c8018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = AveragedPerceptron()\n",
    "loaded_model.load(\"ap.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e590bccd-023d-4e3e-b9c4-48e20f99c704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['QTY',\n",
       " 'UNIT',\n",
       " 'NAME',\n",
       " 'NAME',\n",
       " 'PUNC',\n",
       " 'PREP',\n",
       " 'PREP',\n",
       " 'PREP',\n",
       " 'PREP',\n",
       " 'PREP',\n",
       " 'PREP',\n",
       " 'PREP']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PreProcessor(\"600 g pork tenderloin, trimmed and cut into 10 cm pieces\")\n",
    "tag(loaded_model, p.sentence_features())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
